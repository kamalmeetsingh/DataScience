{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation for RNN  ---- Why RNN??\n",
    "\n",
    "1. Like spatial relationships data can exhibit other relationships that requires higer order tensors. \n",
    "     1.a. relationship order needs to be preserved... \n",
    "     \n",
    "2. *Ordered sequences* is one such examples.\n",
    "\n",
    "    2.a Ordered sequences are sequences that follow a pattern e.g language structure.. \n",
    "    \n",
    "    2.b. Every language has specific structure e.g. in English the first word start with capital letter, there are Noun, Pronouns, parts of speech and rules that govern the order in which the admissible words would come. \n",
    "    \n",
    "3. Time series data is another example of ordered sequences... \n",
    "   \n",
    "   a. Take the time series prediction for stock price. \n",
    "   \n",
    "   b. If we are attempting to predict the stock price based on the *historical* price points we cannot take any past data/sequence...\n",
    "   \n",
    "   Take for example the two time series depicted below -- even though the second time series/stock price can be generated by changing the order of price points\n",
    "   \n",
    "   <img src=\"images/time_series.png\" width=\"50%\">\n",
    "   \n",
    "  this time series is  very different from the one below\n",
    "  \n",
    "  <img src=\"images/time_series_2.png\" width=\"50%\"> \n",
    "\n",
    "  \n",
    "We need to do better to exploit these structures... \n",
    "\n",
    "### Let's construct a wishlist of what we are looking for. \n",
    "\n",
    "1. An Architecture that takes into account the order of sequential structure -- depicted in the data. \n",
    "\n",
    "2. An Architecture that takes into the past predictions/values while predicting the next value in the sequence. \n",
    "\n",
    "3. Preserves all the benifits of fully connected network... \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing RNN's \n",
    "\n",
    "RNNs can come for our recue, an RNN Architecture promises to\n",
    "\n",
    "1. preserve the sequence structure --- i.e the outcome of next prediction is in harmonic with the previous predictions. \n",
    "\n",
    "2. takes into account the last predicted sequence while predicting the next sequence\n",
    "\n",
    "g(S) = W0 + W1*S      ---> represents an RNN...\n",
    "\n",
    "_S1 =   W0 + W1*S0_\n",
    "\n",
    "_S2 =  W0 + W1*S1_\n",
    "\n",
    "_S3 =  W0 + W1*S2_\n",
    "\n",
    "_S4 =  W0 + W1*S3_\n",
    "\n",
    "This is a well known equation --- we have seen this while coming up with fully connected network -- but has a MAJOR difference...\n",
    "\n",
    "S1 S2 S3 .... are calculated at different instance of time as against to fully connected Architecture where all the information was calculated in one pass... \n",
    "\n",
    "What I wrote above has been one of the most intricate problems in Mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent a time sequence below... \n",
    "<img src=\"images/time_windowing.gif\" width=\"50%\"> \n",
    "\n",
    "$$s_{0},s_{1},s_{2},...,s_{P}$$\n",
    "\n",
    "$s_{p}$ is the numerical value of the time series at time period $p$ and $P$ is the total length of the series.\n",
    "\n",
    "RNN will treat the time series prediction problem as a regression problem, and associated the input/output pairs are constructed using sliding window to construct, shown above,  aginst which the regression is performed.\n",
    "\n",
    "For example - using a window of size T = 5 (as illustrated in the gif above) we produce a set of input/output pairs like the one shown in the table below\n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Input} & \\text{Output}\\\\\n",
    "\\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\\\\n",
    "\\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\\\\n",
    "\\color{CornflowerBlue}  {\\vdots} & \\color{Goldenrod} {\\vdots}\\\\\n",
    "\\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}}\n",
    "\\end{array}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "1. Size of the input sequence is the **length** of the sliding window -- 4 in our case... \n",
    "\n",
    "2. The output generated is a scalar value... \n",
    "\n",
    "3. This scalar value becomes the part of the input sequence for next iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN representation... \n",
    "\n",
    "RNNs have two **confusing** representation that which are though verry innocous... \n",
    "\n",
    "<img src=\"images/rnn_both.jpg\" width=\"50%\"> \n",
    "\n",
    "\n",
    "#### There are few things to note \n",
    "\n",
    "1. The diagram on the represents the *folded* or the recurcive form.. \n",
    "\n",
    "2. The folded form *unfolds* to a graphical form... \n",
    "\n",
    "3. x_t is the input at time step t. For example, x_1 could be a one-hot vector corresponding to the second word of a sentence...\n",
    "\n",
    "4. s_t is the hidden state at time step t. It’s the “memory” of the network. s_t is calculated based on the previous hidden state and the input at the current step: s_t=f(Ux_t + Ws_{t-1}).\n",
    "\n",
    " The function f usually is a nonlinearity such as tanh or ReLU.  s_{-1}, which is required to calculate the first hidden state, is typically initialized to all zeroes.\n",
    " \n",
    "5. o_t is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. o_t = \\mathrm{softmax}(Vs_t).\n",
    "\n",
    "#### Deeper thoughts... \n",
    "\n",
    "1. Think of the hidden state s_t as the memory of the network. s_t captures information about what happened in all the previous time steps. The output at step o_t is calculated solely based on the memory at time t.\n",
    "\n",
    "\n",
    "2. Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (U, V, W above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.\n",
    "\n",
    "\n",
    "3. The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. \n",
    "\n",
    "\n",
    "_The main feature of an RNN is its hidden state, which captures some information about a sequence._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case for LSTM\n",
    "\n",
    "Taking forward our yesterday's image classification and adding the context of RNNs that we have discussed so far... The image can represent both dog and wolf but if we are studying carnivores that are non domestic wolf would be more appropiate.. \n",
    "\n",
    "\n",
    "**We need to analyze all the pictures together**\n",
    "\n",
    "<img src=\"images/rnn-lstm.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "so far so good.. RNNs can solve this problem but here comes the catch -- what about scenarios where referance context is far apart... \n",
    "\n",
    "1. RNNs in those cases suffer from vanishing gradiant problem.. \n",
    "\n",
    "2. There is another problem of exploding grading... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vanishing.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Architecture qualitative \n",
    "\n",
    "<img src=\"images/LSTM_architecture.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what goes where  --- \n",
    "\n",
    "1. short Term memory and event goes to Learn gate.\n",
    "2. Long term memory goes to forget gate.. \n",
    "\n",
    "<img src=\"images/LSTM_dynamics.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We pass to Remember gate whatever is relevant... \n",
    "\n",
    "1. From remember gate pass what is useful for the context. \n",
    "\n",
    "2. From Learn gate pass everything.. \n",
    "\n",
    "<img src=\"images/LSTM_interaction.png\" width=\"50%\"> \n",
    "\n",
    "\n",
    "Remember gate this result to new Long term memory for future reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Pass the same info to Use gate for prediction\n",
    "\n",
    "<img src=\"images/LSTM_use_gate.png\" width=\"50%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is the final state\n",
    "\n",
    "<img src=\"images/LSTM_output.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
