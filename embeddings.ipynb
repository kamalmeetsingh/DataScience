{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation for Word Embeddings : Extending One hot encoding\n",
    "\n",
    "1. Treat each word as a class \n",
    "\n",
    "2. Assign 1 where the lable is true and 0 otherwise\n",
    "\n",
    "<img src=\"images/one-hot.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When the word corpous is large -- say an entire Wikipedia dump-- then one hot encoding would not be ideal..   \n",
    "\n",
    "2. When we are intereseted in co occurrence of words then too one hot encoding will fail\n",
    "\n",
    "\n",
    "We need a way to fix the size of the word vector ... this fixing of the size of the word representation along with the desired latent properties is called word embeddings... \n",
    "\n",
    "\n",
    "*_If two words are similar in meaning they should be closer to each other_*\n",
    "\n",
    "<img src=\"images/similar-embeddings.png\" width=\"35%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### practical usage....\n",
    "\n",
    "This idea can be extended further... \n",
    "If two **pairs** words have simiar difference in their meanings the distance of the word vectors should be closer in the embedding space.. \n",
    "\n",
    "<img src=\"images/embedding-distance.png\" width=\"35%\"> \n",
    "\n",
    "\n",
    "This idea is used a lot in Natural language processing to find... \n",
    "\n",
    "1. Synonims\n",
    "\n",
    "2. Analogies\n",
    "\n",
    "3. Classifying words as positive, negative, neutral "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Vectors for word representation -- Glove \n",
    "\n",
    "Tries to optimize the vector representaion of each words using **co-occurance statistics** \n",
    "\n",
    "Step1 \n",
    "\n",
    "1. Calculate the probability of j in the context of i -- i.e. conditional pbt j given i for all i, j in a given corpus \n",
    "\n",
    "2. Store this probability in a matrix called as co-occurrence matrix...  \n",
    "\n",
    "\n",
    "<img src=\"images/glove.png\" width=\"50%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/glove-eval.png\" width=\"50%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/glove-another-rep.png\" width=\"50%\"> \n",
    "\n",
    "                                      we are essentially factorizing co-occurence matrices into two smaller matrices Context and Target and checking for **Maximum likelihood**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why co -occurence pbt \n",
    "\n",
    "cosider 2 context words ice and steam and 2 target words solid and water\n",
    "\n",
    "solid occurs more in context of ice than steam but water ocurrs equally in both ice and steam \n",
    "\n",
    "<img src=\"images/ice-steam.png\" width=\"50%\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is exactly what the co occurence pbt gives\n",
    "\n",
    "in a large corpus p(solid|ice)/p(solid|steam) >> 1 \n",
    "\n",
    "p(water|ice)/p(water|steam) ~= 1\n",
    "\n",
    "and hence the co-relation pbts are working \n",
    "\n",
    "<img src=\"images/glove-final.png\" width=\"50%\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec ---  Transforms words to Vectors... \n",
    "\n",
    "\n",
    "<img src=\"images/word2vec_2.png\" width=\"50%\"> \n",
    "\n",
    "\n",
    "A model that can predict a word given the neighbouring words or neighbouring words given a word can take into account the contextual meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
